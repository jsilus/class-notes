$\{ v^{(1)},v^{(2)},\dots,v^{(k)} \}$ is **linearly independent** if whenever $\emptyset=\alpha_{1}v^{(1)}+\alpha_{2}v^{(2)}+\cdots+\alpha_{k}v^{(k)}$, then $\alpha_{1}=\alpha_{2}=\cdots=\alpha_{k}=0$. Otherwise, it is **linearly dependent**


**Theorem:** Suppose $\{ v^{(1)},\dots,v^{(n)} \}$ is a set of $n$ linearly independent vectors in $\mathbb{R}^{n}$. Then, for any vector $x\in \mathbb{R}^{n}$, $\exists$ a unique $\{ \beta_{1},\dots,\beta_{n} \}$ such that $x=\beta_{1}v^{(1)}+\cdots+\beta_{n}v^{(n)}$.
*Definition:* $\{ v^{(1)},\dots,v^{(n)} \}$ is called a **basis** of $\mathbb{R}^{n}$.

**Theorem:** If $A$ is a matrix where $\lambda_{1},\dots,\lambda_{n}$ are distinct eigenvalues of $A$ with associated eigenvectors $x^{(1)},\dots,x^{(n)}$, then $\{ x^{(1)},\dots,x^{(n)} \}$ is a linearly independent set.

