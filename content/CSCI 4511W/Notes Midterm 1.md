## Chapter 2
- Agent: anything perceiving its environment and acting within it
- Rationality: consequentialism given known knowledge
## Chapter 3
- Uninformed Search
	- breadth-first search
	- uniform-cost search (Dijkstra's)
	- depth-first search
		- depth-limited search
		- iterative deepening
	- bidirectional search

| Criterion     | BFS        | Dijkstra                                  | DFS        | DLS        | ID         | Bidirectional |
| ------------- | ---------- | ----------------------------------------- | ---------- | ---------- | ---------- | ------------- |
| Complete?     | Yes$^{1}$  | Yes$^{1,2}$                               | No         | No         | Yes$^{1}$  | Yes$^{1,4}$   |
| Optimal Cost? | Yes$^{3}$  | Yes                                       | No         | No         | Yes$^{3}$  | Yes$^{3,4}$   |
| Time          | $O(b^{d})$ | $O(b^{1+\lfloor C^{*} /\epsilon\rfloor})$ | $O(b^{m})$ | $O(b^{l})$ | $O(b^{d})$ | $O(b^{d/2})$  |
| Space         | $O(b^{d})$ | $O(b^{1+\lfloor C^{*} /\epsilon\rfloor})$ | $O(bm)$    | $O(bl)$    | $O(bd)$    | $O(b^{d/2})$  |
$b$ is branching factor, $m$ is maximum depth, $d$ is shallowest solution depth (or $m$ if no solution), $l$ is the depth limit. $^{1}$ if $b$ is finite and the state space has a solution or is finite. $^{2}$ if all action costs are $\geq\epsilon>0$. $^{3}$ if action costs are all identical. $^{4}$ if both directions are BFS or uniform cost.

- Informed Search
	- A* search
		- heuristic is **admissible** if it never overestimates
		- **Optimally efficient** - expands the fewest nodes in the worst case
	- Beam search
		- Reduces the frontier of A* by only keeping $k$ best nodes
	- IDA* - iterative deepening for A*

**Effective branching factor**: $N+1=1+b^{*}+(b^{*})^{2}+\cdots+(b^{*})^{d}$
- $b^{*}$ is the effective branching factor
- $N$ is the number of nodes generated by $A^{*}$
- $d$ is the depth of the tree
- smaller is better

## Chapter 4
- Local Search
	- Hill Climbing Search
	- Simulated annealing is adding some random choices occasionally to avoid local maximums
	- Local beam search
		- Randomly generate $k$ initial states
		- At each step, do everything
		- Select the best $k$ states
	- Evolutionary algorithms are like local beam search
		- **crossover point** is the ratio of parent's genes to select
## Chapter 5
- minimax - tree where we switch between picking the minimum and maximum values at each node
- Alpha-beta pruning
	- $\alpha$ is local known maximum, starts at $-\infty$, increases. $\alpha=$ "at least" 
	- $\beta$ is local known minimum, starts at $\infty$, decreases. $\beta=$ "at most"
	- Effectiveness is dependent on order, though result is same
- Heuristic Alpha-beta pruning
	- Cut off the search at certain points by an evaluation function
	- Kinda like A* for minimax
- Expectiminimax
	- minimax but with a computed expected value for stochastic (random) games
## Chapter 6
- Constraint Satisfaction Problems (CSP)
	- Three components
		- $X$ is a set of variables $\{ X_{1},X_{2},\dots,X_{n} \}$
		- $D$ is a set of domains $\{ D_{1},D_{2},\dots,D_{n} \}$, one for each variable
			- each $D_{i}$ stores all possible values for $X_{i}$
		- $C$ is a set of **constraints** that specify allowable combinations of values
			- $C_{j}=\left\langle scope,rel \right\rangle$ where $scope$ is a tuple of variables and $rel$ is a relation
			- Ex: $\left\langle (X_{1},X_{2}),X_{1}>X_{2} \right\rangle$ says $X_{1}$ must be greater than $X_{2}$.
- Hypergraphs
	- Normal graphs for some parts, squares for nodes represent multi constraints
- **Constraint propagation** is using the constraints to reduce the number of legal values
- Node-consistent if all of the values in a variable's domain satisfy its unary constraints
	- Easy to obtain if we use constraint propagation at the beginning to reduce the domains
- Arc-consistent if all of the values in a variable's domain have a solution within it's binary constraint's partners
	- $X_{i}$ is arc-consistent with respect to $X_{j}$ if for every value in the current domain $D_{i}$ there is some value in the domain $D_{j}$ that satisfies the binary constraint on the arc $(X_{i},X_{j})$
	- AC-3 algorithm enforces arc consistency
- Backtracking search
	- Make a tree where each option is cycled through
	- **Minimum-remaining-values heuristic** is choosing the variable with the fewest legal values first (very good first choice)
	- **Degree heuristic** is choosing the variable that will reduce the branching factor (very good tiebreaker)
	- **Least-constraining-value heuristic** is choosing the value that rules out the fewest choices of neighboring values.
	- **Forward checking**: when $X$ is assigned, establish arc consistency. For each unassigned variable $Y$ that is connected to $X$ by a constraint, delete from $Y$'s domain any value that is inconsistent with the value chosen for $X$.
	- **Chronological backtracking** is reducing back to the most recent decision when something fails.
		- this is like DFS
	- **Backjumping** is to determine all of the conflicting values at failure (conflict set) and back up to the most recent one in it.
	- **Conflict directed backjumping** determine the conflicting values and any subsequent values